
## Create a framework for comprehensive testing across multiple levels of user engagement  

Current methods of assessing graphics using measures like accuracy or speed, misses how users actually engage with charts. There are different ways to evaluate visualizations based on user tasks, but do not test multiple methods on the same charts. This proposed work aims to fill that gap by testing the same charts using multiple methods with the same participants, data and testing conditions. This helps us get a clearer, more complete understanding of how chart design affects user experience.

By combining multiple methods of user testing within the same experiment, we can gather information that spans multiple levels of engagement with acceptably small impact on participant cognitive load.


**Augmentation of statistical lineup:**

eg. - combining lineups with direct annotation, lineups with talk-aloud protocal

**Augmenting single-plot studies:**

eg. -  combine numerical estimation, direct annotation , think-aloud or add forced-choice questions 



### Last Week (11 June - 18 June)
- Read reserach papers related to how cognition works in visualization tasks
- Started drawing congition models


### To do list for the next week
- Continue reading more about cognition and perception and drawing models
- Read about research related to multi-modal user testing
